\documentclass[10pt]{article}
\usepackage[margin=1in]{geometry} 
\usepackage{enumerate, xfrac, color, graphicx}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{amsmath,amsthm,amssymb,amsfonts}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{pifont}
\usepackage{listings, courier}
\graphicspath{{figures}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\lstset{breaklines=true, basicstyle=\small\ttfamily, language=R, backgroundcolor=\color{highlight}, stepnumber=5}

\definecolor{highlight}{RGB}{248,248,248}

\begin{document}
	\title{6867 Problem Set 3}
	\maketitle

    \subsubsection*{1. NN Implementation}
     SoftMax output layer guarantees that we output probabilities that sum to 1.
     Cross-entropy loss allows the derivative of loss w.r.t weights to be nicely defined [GUESS]
     Random weights with variance > .5 tend to produce divergence
\end{document}
