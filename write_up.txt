1.1 Hyperparameters

Figures:
- partial derivatives of Gaussian: flatten to 0 at 50 away from target
- 

### Learning Rates
For the 2D Gaussian example, if learning rates are too low, the weights don't move from the initial values, and the convergence criteria are met prematurely, and the weights never change. This is largely because the value of the partial derivative is on the order of 1e-9.

For the quadratic bowl example, the magnitude of the derivatives is on the order or 1e4-1e5, the ideal learning rates are much lower:
between 1/10,000 and .13 in the given example. For learning rates above .13, the weights explodes to -infinity.
Bowl Note: the second derivative is a constant A, here the second derivative is constant and positive.
For the Gaussian, if learning rates are above 1e10 (above 1e10 for the Gaussian), the weights explode orders of magnitude past the mean to values that are given probability 0, by f(x) and whose gradients are so low that the stopping criterion is triggered.


### Starting Weights:
    Let the distance from the observed mean of the starting parameter be called d. Table 1 shows the result of using gradient descent on the gaussian mu parameter. If we start the gaussian close enough to the target value the gradient will be able to escape from 0 probability land and converge on the correct answer. With ideal parameters, and max iterations=1000 the furthest starting point that resulted in succesful termination was -78 (88 away from 10). Values further from the target could not achieve escape velocity.
    For starting values nearer to the target, we see that the number of iterations required to reach convergence is increasing in distance. (Figure 1).



### Convergence Criteria
The gradient descent algorithm converges when the gradients become low enough that the change in weights is below a threshold on successive iterations. If the convergence criteria is large, the algorithm will take more less time to converge but may converge around an answer that is <threshold> away from the target.


### Evolution of Norm of Gradient
In cases where the algorithm converges to the target, the norm of the gradient declines each iteration.
The maximum change in the norm of the gradient is 0, with the smallest, least negative changes in the steps nearest
to convergence. 
Increasing the learning rate will increase the speed with which the norm falls to 0.
The norm can never fall less than the stopping criterion more than twice in a row.

1.2 Effect of step size on numerical gradient's error

- search for `hmart` in notebook
- quad bowl seems to want h >=1 
- gauss likes h=1/10,000 for starting values close to target, 1/100,000 or 1/1,000,000 for values further from target.
- 
1.3 Stochastic Gradient Descent
a. learning rate schedule satisfying Robbins-Monro Conditions {DONE}
b. {DONE}
c. compare SGD to BGD
- SGD takes more iterations to converge (less learning each iteration)
- SGD doesnt really converge lol
- k parameter in Monro Robbins effects how quickly learning rate declines
- tau parameter effects levels

d. Mini-Batch
    SGD bounces around rather than continually approaching target.  



Problem 2: Linear Basis Function Regression 
1. Replicated Bishop Charts
TODO
2. compute SSE given W and M, write derivative of SSE function
3. Run BGD on SSE for values of M, discuss hyperparams. (NEED ANALYTICAL DERIVATIVE OF LOSS w.r.t M?)
4. Cosine writeup 
