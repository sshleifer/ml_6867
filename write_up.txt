1.1 Hyperparameters

Figures:
- partial derivatives of Gaussian: flatten to 0 at 50 away from target
- 

### Learning Rates
For the 2D Gaussian example, if learning rates are too low, the weights don't move from the initial values, and the convergence criteria are met prematurely, and the weights never change. This is largely because the value of the partial derivative is on the order of 1e-9.

For the quadratic bowl example, the magnitude of the derivatives is on the order or 1e4-1e5, the ideal learning rates are much lower:
between 1/10,000 and .13 in the given example. For learning rates above .13, the weights explodes to -infinity.
Bowl Note: the second derivative is a constant A, here the second derivative is constant and positive.
For the Gaussian, if learning rates are above 1e10 (above 1e10 for the Gaussian), the weights explode orders of magnitude past the mean to values that are given probability 0, by f(x) and whose gradients are so low that the stopping criterion is triggered.


### Starting Weights:
    Let the distance from the observed mean of the starting parameter be called d. Table 1 shows the result of using gradient descent on the gaussian mu parameter. If we start the gaussian close enough to the target value the gradient will be able to escape from 0 probability land and converge on the correct answer. With ideal parameters, and max iterations=1000 the furthest starting point that resulted in succesful termination was -78 (88 away from 10). Values further from the target could not achieve escape velocity.
    For starting values nearer to the target, we see that the number of iterations required to reach convergence is increasing in distance. (Figure 1).



### Convergence Criteria
The gradient descent algorithm converges when the gradients become low enough that the change in weights is below a threshold on successive iterations. If the convergence criteria is large, the algorithm will take more less time to converge but may converge around an answer that is <threshold> away from the target.


### Evolution of Norm of Gradient
In cases where the algorithm converges to the target, the norm of the gradient declines each iteration.
The maximum change in the norm of the gradient is 0, with the smallest, least negative changes in the steps nearest
to convergence. 
Increasing the learning rate will increase the speed with which the norm falls to 0.
The norm can never fall less than the stopping criterion more than twice in a row.

1.2 Effect of step size on numerical gradient's error

- search for `hmart` in notebook
- quad bowl seems to want h >=1 
- gauss likes h=1/10,000 for starting values close to target, 1/100,000 or 1/1,000,000 for values further from target.
- 
1.3 Stochastic Gradient Descent
a. learning rate schedule satisfying Robbins-Monro Conditions {DONE}
b. {DONE}
c. compare SGD to BGD
- SGD takes more iterations to converge (less learning each iteration)
- SGD doesnt really converge lol
- k parameter in Monro Robbins effects how quickly learning rate declines
- tau parameter effects levels

d. Mini-Batch
    SGD bounces around rather than continually approaching target.  



Problem 2: Linear Basis Function Regression 
1. Replicated Bishop Charts
https://github.com/sshleifer/ml_6867/blob/master/figures/2.1M10.png
code in python/basis_func.py
2. Compute SSE given W and M, write derivative of SSE function
code in  notebook.
- Getting 0 error for fairly large step-sizes. 
- Gradients are linear in X for a quadratic of any order, so analytical, numerical gradients are equal for all reasonable step sizes.


3. Run BGD on SSE for values of M, discuss hyperparams.
**SGD vs. BGD:**
- SGD needs a higher learning rate to converge to 2.5 for all values of M.
- Lower stopping criteria reduce the probability of stopping, regardless of whether the algorithm is stuck at a local minima or at the target.



4. Cosine weights are close but not identical to the true coefficients. Fitting a 9 parameter model to 11 variables of data results, as expected in over-fitting. The learned solution is has lower L2 loss than the true data generation process. We need a sparsity penalty to the loss function.
Plots in figures/2.4_weights.png


3.2
Training on dataset A and testing on dataset B suggests a few options:
-   a  model with M=1 pr M=2 and high lambda (roughly 9)
-  

Writeup 2

Writeup 3 

\subsubsection*{Implementing Ridge Regression}

Given the problems we faced for high values of M when learning the Cosine basis function, we will now add 
Let's now see how regression behaves when we impose a regularization term. The regularization term, $\lambda$, serves to make sure we don't overfit to our training data. We implemented the closed form solution of Ridge Regression in Python, making use of Scipy and Numpy. Note that if you want to test cases where the number of features is greater than the number of data points (something that a well-trained teacher of machine should be skeptical to do), it is useful to use the pseudo-inverse, rather than the inverse to fit your parameters. We learned this the long way after debugging code for hours.

The closed form solution for ridge regression is given by the expression:

\begin{equation}
\hat{\theta} = (\mathbf{X}^T\mathbf{X} + \lambda \mathbf{I})^{-1}\mathbf{X}^t\mathbf{y}
\end{equation}

\noindent where $\mathbf{X}$ is your matrix of features, $y$ is your array of observed values, and $\hat{\theta}$ are your estimated feature weights. $\lambda$ is a regularization parameter, which represents how much you penalize the model for having large feature weights.

Given the data in Bishop 1.4 (which can be seen below), we attempt to fit polynomials of various orders, $M$.  The plot below shows the mean squared error (MSE) that our ridge regression fit gives for values of $M$ between 1 and 5, for values of $\lambda$ ranging from $(10^0 -1)$ to $10^2$. Note that in the case where $\lambda = 0$, we have $OLS$ regression, and as $M$ increases, the model is free to overfit our data. While the MSE on training data is low, if we evaluated this on a new data point, our model would likely perform very poorly. As $\lambda$ gets higher, we penalize the model for having large $\hat{\theta}$ components. Our $MSE$s for these models are relatively large. However, these "large" MSE models are likely to perform better on a new data point!

\begin{figure}[ht]
	\centering
	\begin{minipage}[b]{.48\linewidth}
                \includegraphics[width=1\linewidth]{Bishop_14_Data.png}
		\caption*{Original Bishop Data}
	\end{minipage}
	\begin{minipage}[b]{.48\linewidth}
                \includegraphics[width=1\linewidth]{MSE_Lambda_Bishop.png}
		\caption*{$MSE$($M$,$\lambda$)}
	\end{minipage}
\end{figure}

In order to avoid overfitting, while making sure that we have chosen values of $\lambda$ and $M$ that make sense, it is common to split the data into a training set and a test set. We will use the closed-form solution for ridge regression to calculate $\hat{\theta}$ on our training set. We will then use that $\hat{\theta}$ to attempt to predict values of $\mathbf{y}$ for the test data. By choosing the $\hat{\theta}$ that is trained on the training data and minimizes MSE on the test data, we are ensuring that we have not overfit our model to the data it is trained on. As one final step in the model selection process, we can check the performance of our final $\hat{\theta}$ on a validation set, to make sure that performance is still good (and we have not overfit to the data in on our test set).

We use this methodology to find the best $\lambda$ and $M$ given three datasets similar to the bishop data. We calculate $\lambda$ and $M$ using the regress\_train.txt, data we validate using regress\_test.txt, and then we test using regress\_validate.txt. We search through a grid of various values for $\lambda$ and $M$, and find that $MSE$ is lowest on the validation set when $\lambda = 1$ and $M = 1$: $MSE = 1.776$ on the validation data set. These parameters gave us an $MSE$ of 1.460 on the training data and and an $MSE$ of 1.415 on the test data. The fact that our $MSE$ is on the same order of magnitude for the test data is a good sign - it indicates that in making sure we didn't overfit to the training data, we didn't accidentally overfit to the validation data.

The $MSE$ for various combinations of $\lambda$ and $M$ on the three datasets (including our chosen set of hyperparameters) is found below. We see that with large $M$ and small $\lambda$, we do great on the training data, but bad on our validation data. For large values of both $\lambda$ and $M$, we do well, but not as well as our optimal values we found through the grid search.

Let's now use this methodology on a real dataset, as opposed to a toy dataset. The dataset that we will use was compiled by Kirsztian Buza at Budapest University of Technology and Economics. It includes 280 numerical values describing a number of blog posts, such as day of week, number of parent pages, and number of comments received within 24 hours. We are hoping to predict the number of comments it will receive in the next 24 hours. This data is split into training, validation, and test datasets, so we will once again perform a grid search to find the value of $\lambda$ that minimizes $MSE$ on our validation set. In this particular case, we are not regressing a polynomial of arbitrary order, so we do not need to test different values of $M$.

We find that for very large $\lambda$ (as large as $10^6$, $MSE$ continues to decrease, although it is always close to 900. A predictor that uniformly predicts the average value of $y$, $\hat{y}$, has an $MSE$ of 1453, so the performance of our model is still better. However, we see that for values of $\lambda$ higher than $10^6$ or so, the value of the MSE increases, and eventually approaches (and eclipses) the MSE of our uniform estimator.

This isn't surprising - the blog feedback dataset is large, with 10,480 observations in the test set. If we interpret the value of $\lambda$ as the strength of our Bayesian prior on our model parameters, it makes sense that our choice of $\lambda$ does not have a substantial effect unless it is significantly larger than the size of our data. Furthermore, the inability of our model to get very low MSE for low $\lambda$ speaks both to the fact that the data has a much higher dimension than our training data, and also suggests that the simple linear model we are using here may not be a very good fit for the data.

\begin{figure}[ht]
	\centering
	\begin{minipage}[b]{.48\linewidth}
                \includegraphics[width=1\linewidth]{MSE_comparison.png}
                \caption*{$MSE$ across different subsets of toy data}                
	\end{minipage}
	\begin{minipage}[b]{.48\linewidth}
                \includegraphics[width=1\linewidth]{BlogFeedbackRegressionMSE.png}
                \caption*{$MSE$($\lambda$) for blog data}
	\end{minipage}
\end{figure}

